# AI CORE

Conversational AI vá»›i tÃ­nh cÃ¡ch vÃ  nháº­n thá»©c ngá»¯ cáº£nh.

## ğŸ¯ Äáº·c Ä‘iá»ƒm

- **Tá»± nhiÃªn**: NÃ³i chuyá»‡n nhÆ° ngÆ°á»i tháº­t, cÃ³ duyÃªn, biáº¿t Ä‘Ã¹a
- **Trung thá»±c**: KhÃ´ng bá»‹a kiáº¿n thá»©c, thá»«a nháº­n khi khÃ´ng biáº¿t
- **ThÃ´ng minh**: Tá»± nháº­n biáº¿t ngá»¯ cáº£nh Ä‘á»ƒ Ä‘iá»u chá»‰nh giá»ng Ä‘iá»‡u
- **Má»Ÿ rá»™ng**: Dá»… dÃ ng thÃªm tools, models, vÃ  knowledge

## ğŸ—ï¸ Kiáº¿n trÃºc

```
User Input
   â†“
Context Analyzer (phÃ¢n tÃ­ch ngá»¯ cáº£nh)
   â†“
Persona Selector (chá»n tÃ­nh cÃ¡ch)
   â†“
Memory Loader (load lá»‹ch sá»­)
   â†“
Prompt Builder (xÃ¢y prompt)
   â†“
Model Client (gá»i LLM)
   â†“
Output Processor (xá»­ lÃ½ output)
   â†“
Response
```

## ğŸ“¦ CÃ i Ä‘áº·t

```bash
# Clone repo
git clone <repo-url>
cd ai-core

# Install dependencies
pip install -r requirements.txt
```

## ğŸš€ Cháº¡y

```bash
# Start API server
python main.py

# Server sáº½ cháº¡y táº¡i http://localhost:8000
```

## ğŸ“¡ API Endpoints

### POST /chat
Gá»­i tin nháº¯n vÃ  nháº­n pháº£n há»“i

```json
{
  "message": "Xin chÃ o!",
  "session_id": "optional-session-id"
}
```

### POST /chat/new-session
Táº¡o session má»›i

### GET /chat/history/{session_id}
Láº¥y lá»‹ch sá»­ chat

### DELETE /chat/session/{session_id}
XÃ³a session

## ğŸ¨ Personas

- **Casual**: Thoáº£i mÃ¡i, vui váº», Ä‘Ã¹a giá»¡n
- **Technical**: NghiÃªm tÃºc, chÃ­nh xÃ¡c, chi tiáº¿t
- **Cautious**: Cáº©n tháº­n, thá»«a nháº­n khi khÃ´ng biáº¿t

AI tá»± Ä‘á»™ng chá»n persona dá»±a trÃªn ngá»¯ cáº£nh.

## ğŸ› ï¸ Cáº¥u hÃ¬nh

Chá»‰nh sá»­a file trong `app/config/`:
- `persona.yaml` - Cáº¥u hÃ¬nh tÃ­nh cÃ¡ch
- `rules.yaml` - Quy táº¯c xá»­ lÃ½
- `system.yaml` - Cáº¥u hÃ¬nh há»‡ thá»‘ng

## ğŸ”§ Sá»­ dá»¥ng model khÃ¡c

Máº·c Ä‘á»‹nh dÃ¹ng mock model Ä‘á»ƒ test. Äá»ƒ dÃ¹ng model tháº­t:

```python
from app.model import ModelClient
from app.core import AICore

# OpenAI
model = ModelClient(
    provider="openai",
    api_key="your-key",
    model_name="gpt-4"
)

# Anthropic
model = ModelClient(
    provider="anthropic",
    api_key="your-key",
    model_name="claude-3-sonnet"
)

# Local model
model = ModelClient(
    provider="local",
    base_url="http://localhost:8080",
    model_name="llama-3-8b"
)

ai = AICore(model_client=model)
```

## ğŸ“š ThÆ° má»¥c

```
ai-core/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/          # FastAPI endpoints
â”‚   â”œâ”€â”€ core/         # AI Core logic
â”‚   â”œâ”€â”€ memory/       # Memory management
â”‚   â”œâ”€â”€ model/        # Model clients
â”‚   â”œâ”€â”€ tools/        # Tool system
â”‚   â””â”€â”€ config/       # Configuration files
â”œâ”€â”€ data/             # Data storage
â”œâ”€â”€ tests/            # Tests
â””â”€â”€ main.py           # Entry point
```

## ğŸ§ª Testing

```bash
# Test vá»›i mock model
python main.py

# Gá»­i request test
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Xin chÃ o!"}'
```

## ğŸ“ License

MIT

## ğŸ¤ Contributing

Pull requests welcome!
