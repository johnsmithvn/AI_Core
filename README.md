# AI CORE

Conversational AI vá»›i tÃ­nh cÃ¡ch vÃ  nháº­n thá»©c ngá»¯ cáº£nh.

## ğŸ¯ Äáº·c Ä‘iá»ƒm

- **Tá»± nhiÃªn**: NÃ³i chuyá»‡n nhÆ° ngÆ°á»i tháº­t, cÃ³ duyÃªn, biáº¿t Ä‘Ã¹a
- **Trung thá»±c**: KhÃ´ng bá»‹a kiáº¿n thá»©c, thá»«a nháº­n khi khÃ´ng biáº¿t
- **ThÃ´ng minh**: Tá»± nháº­n biáº¿t ngá»¯ cáº£nh Ä‘á»ƒ Ä‘iá»u chá»‰nh giá»ng Ä‘iá»‡u
- **Má»Ÿ rá»™ng**: Dá»… dÃ ng thÃªm tools, models, vÃ  knowledge

## ğŸ—ï¸ Kiáº¿n trÃºc

```
User Input
   â†“
Context Analyzer (phÃ¢n tÃ­ch ngá»¯ cáº£nh)
   â†“
Persona Selector (chá»n tÃ­nh cÃ¡ch)
   â†“
Memory Loader (load lá»‹ch sá»­)
   â†“
Prompt Builder (xÃ¢y prompt)
   â†“
Model Client (gá»i LLM)
   â†“
Output Processor (xá»­ lÃ½ output)
   â†“
Response
```

## ğŸ“¦ CÃ i Ä‘áº·t

```bash
# Clone repo
git clone <repo-url>
cd ai-core

# Táº¡o virtual environment (khuyáº¿n nghá»‹)
python -m venv venv

# KÃ­ch hoáº¡t venv
# Windows:
venv\Scripts\activate
# Linux/Mac:
# source venv/bin/activate

# CÃ i dependencies (trong venv)
pip install -r requirements.txt
```

**LÆ°u Ã½**: LuÃ´n activate venv trÆ°á»›c khi cháº¡y:
```bash
venv\Scripts\activate  # Windows
python main.py
```

## ğŸš€ Cháº¡y

```bash
# Start API server
python main.py

# Server sáº½ cháº¡y táº¡i http://localhost:8000
```

## ğŸ“¡ API Endpoints

### POST /chat
Gá»­i tin nháº¯n vÃ  nháº­n pháº£n há»“i

```json
{
  "message": "Xin chÃ o!",
  "session_id": "optional-session-id"
}
```

### POST /chat/new-session
Táº¡o session má»›i

### GET /chat/history/{session_id}
Láº¥y lá»‹ch sá»­ chat

### DELETE /chat/session/{session_id}
XÃ³a session

## ğŸ¨ Personas

- **Casual**: Thoáº£i mÃ¡i, vui váº», Ä‘Ã¹a giá»¡n
- **Technical**: NghiÃªm tÃºc, chÃ­nh xÃ¡c, chi tiáº¿t
- **Cautious**: Cáº©n tháº­n, thá»«a nháº­n khi khÃ´ng biáº¿t

AI tá»± Ä‘á»™ng chá»n persona dá»±a trÃªn ngá»¯ cáº£nh.

## ğŸ› ï¸ Cáº¥u hÃ¬nh

Chá»‰nh sá»­a file trong `app/config/`:
- `persona.yaml` - Cáº¥u hÃ¬nh tÃ­nh cÃ¡ch
- `rules.yaml` - Quy táº¯c xá»­ lÃ½
- `system.yaml` - Cáº¥u hÃ¬nh há»‡ thá»‘ng

## ğŸ”§ Chá»n Model Provider

Máº·c Ä‘á»‹nh dÃ¹ng **mock model** Ä‘á»ƒ test. Äá»ƒ dÃ¹ng model tháº­t, chá»‰nh sá»­a file `.env`:

### BÆ°á»›c 1: Copy file config máº«u
```bash
cp .env.example .env
```

### BÆ°á»›c 2: Chá»n provider trong `.env`

**Option 1: OpenAI (GPT-4)**
```bash
MODEL_PROVIDER=openai
OPENAI_API_KEY=sk-your-key-here
OPENAI_MODEL=gpt-4  # hoáº·c gpt-3.5-turbo
```

**Option 2: Anthropic (Claude)**
```bash
MODEL_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-your-key-here
ANTHROPIC_MODEL=claude-3-sonnet-20240229
```

**Option 3: Local Model (llama.cpp/vLLM/Ollama)**
```bash
MODEL_PROVIDER=local
LOCAL_MODEL_URL=http://localhost:8080
LOCAL_MODEL_NAME=llama-3-8b
```

**Option 4: Mock (default)**
```bash
MODEL_PROVIDER=mock
```

### BÆ°á»›c 3: Restart server
```bash
python main.py
# Log sáº½ hiá»‡n: "AI Core initialized with provider: openai"
```

## ğŸ“š ThÆ° má»¥c

```
ai-core/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/          # FastAPI endpoints
â”‚   â”œâ”€â”€ core/         # AI Core logic
â”‚   â”œâ”€â”€ memory/       # Memory management
â”‚   â”œâ”€â”€ model/        # Model clients
â”‚   â”œâ”€â”€ tools/        # Tool system
â”‚   â””â”€â”€ config/       # Configuration files
â”œâ”€â”€ data/             # Data storage
â”œâ”€â”€ tests/            # Tests
â””â”€â”€ main.py           # Entry point
```

## ğŸ§ª Testing

```bash
# Test vá»›i mock model
python main.py

# Gá»­i request test
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Xin chÃ o!"}'
```

## ğŸ“ License

MIT

## ğŸ¤ Contributing

Pull requests welcome!
