# CHANGELOG

All notable changes to AI Core will be documented in this file.

## [Unreleased]

## [2.1.0] - 2026-02-01

### ‚ö†Ô∏è BREAKING CHANGES - Response Structure (UI c·∫ßn c·∫≠p nh·∫≠t)

#### 1. Metadata field changes
```diff
# Context analysis response
- "confidence": 0.5          # ƒê√£ deprecated
+ "signal_strength": 0.5     # M·ª©c ƒë·ªô t√≠n hi·ªáu keyword (KH√îNG ph·∫£i x√°c su·∫•t)
+ "context_clarity": true    # true = r√µ r√†ng, false = conflict gi·ªØa casual/technical
+ "confidence": 0.5          # V·∫´n gi·ªØ cho backward compatibility
```

#### 2. Full response metadata structure (t·ª´ /chat endpoint)
```json
{
  "response": "...",
  "session_id": "xxx",
  "metadata": {
    "persona_used": "Casual + Cautious",
    "tone": "casual",
    "behavior": "cautious",
    "context_type": "casual",
    "needs_knowledge": true,
    "signal_strength": 0.5,      // NEW: Thay th·∫ø confidence
    "context_clarity": true,     // NEW: C√≥ conflict kh√¥ng
    "confidence": 0.5,           // DEPRECATED: Gi·ªØ cho backward compat
    "length": 150,
    "word_count": 25,
    "estimated_read_time": 1,
    "has_code_blocks": false
  }
}
```

### Added
- **`signal_strength`** - M·ª©c ƒë·ªô t√≠n hi·ªáu keyword (0-1)
  - 0 = kh√¥ng c√≥ keyword n√†o match
  - 0.5 = 1 keyword match
  - 0.67 = 2 keywords match
  - **L∆ØU √ù**: KH√îNG ph·∫£i x√°c su·∫•t ƒë√∫ng, ch·ªâ l√† signal strength
  
- **`context_clarity`** - C√≥ r√µ r√†ng kh√¥ng
  - `true` = ch·ªâ casual HO·∫∂C technical c√≥ signal (r√µ r√†ng)
  - `false` = c·∫£ 2 ƒë·ªÅu c√≥ signal (conflict) ho·∫∑c ƒë·ªÅu kh√¥ng c√≥

- **Test automation** - 30 test cases
  - `tests/test_conversations.yaml` - 20 core tests
  - `tests/test_edge_cases.yaml` - 10 edge tests v·ªõi `must_pass` / `allowed_to_fail`
  - `tests/test_automation.py` - Script t·ª± ƒë·ªông ch·∫°y test

### Changed
- **Score formula** (internal - kh√¥ng ·∫£nh h∆∞·ªüng API response)
  - C≈©: `matches / total_keywords` ‚Üí list keyword d√†i = score th·∫•p (bug)
  - M·ªõi: `matches / (matches + 1)` ‚Üí 1 match lu√¥n = 0.5

### Deprecated
- **`confidence`** trong metadata - V·∫´n ho·∫°t ƒë·ªông nh∆∞ng n√™n d√πng `signal_strength`

### Notes for UI
1. **N√™n hi·ªÉn th·ªã `context_clarity`** khi debug/admin mode
2. **Kh√¥ng n√™n hi·ªÉn th·ªã `signal_strength`** cho end-user (d·ªÖ hi·ªÉu nh·∫ßm l√† "ƒë·ªô ch·∫Øc ch·∫Øn")
3. **Backward compatible** - `confidence` v·∫´n c√≥ trong response

---

## [2.0.0] - 2026-02-01

### Added - **Tone + Behavior Architecture** (NEW)
- **T√°ch bi·ªát Tone v√† Behavior** thay v√¨ ch·ªçn 1 persona c·ª©ng
  - `tone`: casual | technical (c√°ch n√≥i - quy·∫øt ƒë·ªãnh b·ªüi context_type)
  - `behavior`: normal | cautious (h√†nh vi - quy·∫øt ƒë·ªãnh b·ªüi needs_knowledge)
  - 4 combinations: casual+normal, casual+cautious, technical+normal, technical+cautious
  
- **Vui v·∫ª NH∆ØNG kh√¥ng b·ªãa** - k·∫øt h·ª£p tone casual v·ªõi behavior cautious
  - Tr∆∞·ªõc: "T√¥i kh√¥ng c√≥ th√¥ng tin c·ª• th·ªÉ..." (ƒë√∫ng nh∆∞ng kh√¥ khan)
  - Sau: "√îi gu b·∫°n ch·∫•t ƒë·∫•y! üòÑ N√≥i th·∫≠t m√¨nh kh√¥ng r√†nh, th·ª≠ h·ªèi th∆∞ vi·ªán ƒëi!"

### Changed
- **persona.yaml** - C·∫•u tr√∫c m·ªõi v·ªõi `tones:` v√† `behaviors:` sections
- **PersonaSelector** - `select()` build persona ƒë·ªông t·ª´ tone_config + behavior_config
- **OutputProcessor** - Metadata m·ªõi: `tone`, `behavior` thay v√¨ `response_mode`
- **rules.yaml** - Th√™m keywords technical, gi·∫£m threshold need_knowledge xu·ªëng 0.1

### Improved
- T·ª± nhi√™n h∆°n khi AI kh√¥ng bi·∫øt nh∆∞ng v·∫´n vui v·∫ª
- Flexible persona system cho future extensions
- Legacy support cho code c≈© (personas section v·∫´n ho·∫°t ƒë·ªông)

### Notes
- `temperature` ch·ªâ l·∫•y t·ª´ tone (ƒë√∫ng v·ªÅ b·∫£n ch·∫•t - temperature = style)

---

## [1.2.0] - 2026-02-01

### Changed - **Length Management Philosophy** (BREAKING: Behavioral change)
- ‚ùå **REMOVED hard truncate after generation** (anti-pattern cho local AI)
  - C·∫Øt text sau khi model generate = v√¥ nghƒ©a (ƒë√£ t·ªën t√†i nguy√™n)
  - `max_length: null` in `rules.yaml` - kh√¥ng gi·ªõi h·∫°n
  
- ‚úÖ **NEW: Content description, not control**
  - `output.py` gi·ªù ch·ªâ **m√¥ t·∫£ content** qua metadata
  - Added: `word_count`, `estimated_read_time`, `has_code_blocks`
  - Length validation ‚Üí behavior warnings (kh√¥ng c·∫Øt text)
  
- ‚úÖ **NEW: Context-aware length behavior validation**
  - Casual chat d√†i >3000 chars ‚Üí warning
  - Cautious + d√†i + certainty ‚Üí suspicious
  - Low confidence + very long ‚Üí warning
  - **Philosophy**: Validate behavior, not truncate output

- ‚úÖ **NEW: AI self-managed response length**
  - Updated `BASE_SYSTEM_PROMPT` v·ªõi length management guideline
  - AI c√≥ th·ªÉ t√≥m t·∫Øt tr∆∞·ªõc, h·ªèi user mu·ªën chi ti·∫øt kh√¥ng
  - Response >500 t·ª´ ‚Üí chia nh·ªè ho·∫∑c h·ªèi user
  - Gi·ªëng c√°ch ng∆∞·ªùi th·∫≠t n√≥i chuy·ªán

### Fixed - **Semantic Corrections**
- **`context_type` vs `response_mode` separation** (context.py)
  - `context_type`: casual | technical (what user is asking)
  - `response_mode`: casual | technical | cautious (how AI responds)
  - Before: `cautious` was mixed with `need_knowledge` ‚Üí semantic error
  
- **User message `persona=None`** (engine.py)
  - User messages don't have persona, only assistant messages do
  - Before: `persona=persona["name"]` for user ‚Üí incorrect schema

### Improved
- **Separation of concerns**: AI Core m√¥ t·∫£, UI quy·∫øt ƒë·ªãnh hi·ªÉn th·ªã
- Better metadata cho frontend: read time, word count, response_mode
- More natural conversation flow
- Cleaner semantic model

## [1.1.3] - 2026-01-25

### Added
- **Dynamic server configuration** via environment variables
  - `API_HOST` - Configure listen address (default: 0.0.0.0)
  - `API_PORT` - Configure port (default: 8000)
  - `API_RELOAD` - Enable auto-reload for development (default: false)
- **Complete API Documentation** - Created `docs/API_REFERENCE.md`
  - 7 endpoints fully documented v·ªõi examples
  - Request/response schemas
  - curl, Python, JavaScript examples
  - Status codes v√† error handling
- Improved startup logging v·ªõi configuration display

### Changed
- `main.py` now reads config from `.env` file
- More flexible deployment (Docker, cloud, local)
- Documentation structure improved

### Fixed
- Hard-coded port 8000 ‚Üí Now configurable
- Missing API documentation

## [1.1.2] - 2026-01-25

### Fixed
- **OpenAI API compliance**: `_openai_complete()` gi·ªù tu√¢n th·ªß 100% OpenAI API docs
  - Th√™m `stream: False` explicit parameter
  - Response validation tr∆∞·ªõc khi parse (check choices, message, content)
  - Parse OpenAI error messages correctly t·ª´ response JSON
  - Better error handling: HTTPStatusError, TimeoutException v·ªõi messages r√µ r√†ng
  - Default values cho usage khi API kh√¥ng tr·∫£ v·ªÅ

- **Local model API improvements**: `_local_complete()` enhanced cho LM Studio/Ollama/vLLM
  - Th√™m `stream: False` v√† proper headers
  - Minimum timeout 60s (local models ch·∫≠m h∆°n cloud)
  - ConnectError handling v·ªõi helpful message "is the server running?"
  - Response validation ƒë·∫ßy ƒë·ªß nh∆∞ OpenAI
  - Specific error messages cho t·ª´ng error type

### Improved
- Error messages gi·ªù r√µ r√†ng h∆°n, gi√∫p debug nhanh
- Validate response structure tr∆∞·ªõc khi access fields (tr√°nh KeyError)
- Timeout messages nh·∫Øc user check model loaded

## [1.1.1] - 2026-01-25

### Fixed
- Local model endpoint confirmed using `/v1/chat/completions` (OpenAI-compatible)
- Full support for LM Studio, Ollama, vLLM, llama.cpp

### Changed
- Clarified **Hybrid Architecture** in documentation
- AI Core = Framework (core logic) + Model Abstraction (flexible providers)
- Core logic independent: context/persona/prompt/output
- Model layer: abstraction cho nhi·ªÅu providers

### Documentation
- README.md: Th√™m hybrid approach explanation
- QUICK_START.md: LM Studio default port 1234
- .env.example: Specific configs cho LM Studio/Ollama/vLLM
- CODEBASE_ANALYSIS.md: Clarify architecture design

## [1.1.0] - 2026-01-25

### Added
- Environment-based provider selection via `.env` file
- `.env.example` template file with 4 provider options
- `python-dotenv` dependency for environment variables
- Automatic provider selection in `app/api/chat.py`
- Support for MODEL_PROVIDER env var (mock/openai/anthropic/local)
- Structured logging with provider info on startup

### Changed
- Model provider selection now via `.env` instead of code changes
- Updated README.md with `.env` configuration guide
- Updated QUICK_START.md with simplified setup (4 options)
- Updated STRUCTURE.md with `.env.example` reference
- Simplified developer experience - no code changes needed

### Fixed
- QUICK_START.md section numbering (was 1,2,3,6,6,7,8 ‚Üí now 0-7)
- Documentation consistency across all files

## [1.0.0] - 2026-01-25

### Added
- Initial project structure
- Core AI engine with context awareness
- Persona system (casual, technical, cautious)
- Short-term and long-term memory
- Model client abstraction (OpenAI, Anthropic, local, mock)
- FastAPI REST API
- Configuration system with YAML
- Tool system foundation
- Context analyzer for smart response selection
- Output processor with validation
- Prompt builder with dynamic system prompts

### Core Components
- `app/core/engine.py` - Main AI Core orchestrator
- `app/core/context.py` - Context analysis
- `app/core/persona.py` - Persona selection
- `app/core/prompt.py` - Prompt building
- `app/core/output.py` - Output processing
- `app/memory/` - Memory management
- `app/model/client.py` - LLM client
- `app/api/chat.py` - REST API

### Configuration
- `app/config/persona.yaml` - Persona definitions
- `app/config/rules.yaml` - Core rules and detection
- `app/config/system.yaml` - System settings

### Documentation
- README.md with installation and usage
- TODO.md for tracking progress
- Architecture documentation in code
