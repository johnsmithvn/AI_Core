# AI Core Configuration
# Copy this file to .env and fill in your values

# ============================================
# MODEL PROVIDER SELECTION
# ============================================
# Options: mock, openai, anthropic, local
# Default: mock (no API key needed, for testing)
MODEL_PROVIDER=mock

# ============================================
# OPTION 1: OpenAI Configuration
# ============================================
# Uncomment these lines if using MODEL_PROVIDER=openai
# OPENAI_API_KEY=sk-your-openai-key-here
# OPENAI_MODEL=gpt-4  # or gpt-3.5-turbo, gpt-4-turbo, etc.

# ============================================
# OPTION 2: Anthropic Configuration
# ============================================
# Uncomment these lines if using MODEL_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
# ANTHROPIC_MODEL=claude-3-sonnet-20240229  # or claude-3-opus, claude-3-haiku

# ============================================
# OPTION 3: Local Model Configuration
# ============================================
# Uses OpenAI-compatible API (/v1/chat/completions)
# Uncomment ONE of these configurations:

# LM Studio (default port 1234)
# LOCAL_MODEL_URL=http://localhost:1234
# LOCAL_MODEL_NAME=mistral-7b

# Ollama (default port 11434)
# LOCAL_MODEL_URL=http://localhost:11434
# LOCAL_MODEL_NAME=llama3

# vLLM / llama.cpp (custom port)
# LOCAL_MODEL_URL=http://localhost:8080
# LOCAL_MODEL_NAME=your-model

# ============================================
# QUICK SETUP EXAMPLES
# ============================================

# Example 1: Use OpenAI GPT-4
# MODEL_PROVIDER=openai
# OPENAI_API_KEY=sk-...
# OPENAI_MODEL=gpt-4

# Example 2: Use Anthropic Claude
# MODEL_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-...
# ANTHROPIC_MODEL=claude-3-sonnet-20240229

# Example 3: Use Local Model (llama.cpp, vLLM, Ollama)
# MODEL_PROVIDER=local
# LOCAL_MODEL_URL=http://localhost:8080
# LOCAL_MODEL_NAME=llama-3-8b

# Example 4: Use Mock (default, no config needed)
# MODEL_PROVIDER=mock
