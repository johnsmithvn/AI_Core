# AI Core Configuration
# Copy this file to .env and fill in your values

# ============================================
# API SERVER CONFIGURATION
# ============================================
API_HOST=0.0.0.0           # Listen on all network interfaces (0.0.0.0) or localhost only (127.0.0.1)
API_PORT=8000              # Default port (change if conflict with other services)
API_RELOAD=false           # Enable auto-reload in development (true/false)

# ============================================
# MODEL PROVIDER SELECTION
# ============================================
# Options: mock, openai, anthropic, local
# Default: mock (no API key needed, for testing)
MODEL_PROVIDER=mock

# ============================================
# OPTION 1: OpenAI Configuration
# ============================================
# Uncomment these lines if using MODEL_PROVIDER=openai
# OPENAI_API_KEY=sk-your-openai-key-here
# OPENAI_MODEL=gpt-4  # or gpt-3.5-turbo, gpt-4-turbo, etc.

# ============================================
# OPTION 2: Anthropic Configuration
# ============================================
# Uncomment these lines if using MODEL_PROVIDER=anthropic
# MODEL_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
# ANTHROPIC_MODEL=claude-3-sonnet-20240229  # or claude-3-opus, claude-3-haiku


# ============================================
# OPTION 3: Local Model Configuration
# ============================================
# Uncomment these lines if using MODEL_PROVIDER=local
# MODEL_PROVIDER=local
# LOCAL_MODEL_URL=http://127.0.0.1:1234
# LOCAL_MODEL_NAME=auto  # "auto" = get model name from server response or specify model name example: "gemini-1.5"
# LOCAL_TIMEOUT=300     # Timeout in seconds (default 300s for slow models)


