# AI Core Configuration
# Copy this file to .env and fill in your values

# ============================================
# MODEL PROVIDER SELECTION
# ============================================
# Options: mock, openai, anthropic, local
# Default: mock (no API key needed, for testing)
MODEL_PROVIDER=mock

# ============================================
# OPTION 1: OpenAI Configuration
# ============================================
# Uncomment these lines if using MODEL_PROVIDER=openai
# OPENAI_API_KEY=sk-your-openai-key-here
# OPENAI_MODEL=gpt-4  # or gpt-3.5-turbo, gpt-4-turbo, etc.

# ============================================
# OPTION 2: Anthropic Configuration
# ============================================
# Uncomment these lines if using MODEL_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
# ANTHROPIC_MODEL=claude-3-sonnet-20240229  # or claude-3-opus, claude-3-haiku

# ============================================
# OPTION 3: Local Model Configuration
# ============================================
# Uncomment these lines if using MODEL_PROVIDER=local
# LOCAL_MODEL_URL=http://localhost:8080
# LOCAL_MODEL_NAME=llama-3-8b  # or your model name

# ============================================
# QUICK SETUP EXAMPLES
# ============================================

# Example 1: Use OpenAI GPT-4
# MODEL_PROVIDER=openai
# OPENAI_API_KEY=sk-...
# OPENAI_MODEL=gpt-4

# Example 2: Use Anthropic Claude
# MODEL_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-...
# ANTHROPIC_MODEL=claude-3-sonnet-20240229

# Example 3: Use Local Model (llama.cpp, vLLM, Ollama)
# MODEL_PROVIDER=local
# LOCAL_MODEL_URL=http://localhost:8080
# LOCAL_MODEL_NAME=llama-3-8b

# Example 4: Use Mock (default, no config needed)
# MODEL_PROVIDER=mock
